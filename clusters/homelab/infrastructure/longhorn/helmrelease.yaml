---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: longhorn
  namespace: longhorn-system
spec:
  interval: 1h
  chart:
    spec:
      chart: longhorn
      version: "1.7.x"
      sourceRef:
        kind: HelmRepository
        name: longhorn
        namespace: longhorn-system
  values:
    # Persistence settings for 2-node HA
    persistence:
      defaultClass: true
      defaultClassReplicaCount: 2  # 2 replicas for 2-node HA
      defaultDataLocality: best-effort  # Try to keep replica on same node as workload
      reclaimPolicy: Retain
      migratable: true  # Enable live migration
      recurringJobSelector:
        enable: false
      defaultFsType: ext4
      defaultMkfsParams: ""
      defaultNodeSelector:
        enable: false  # Disable for now, will use node tags in Longhorn
        selector: ""
      defaultDiskSelector:
        enable: false
        selector: ""
      
    # CSI settings
    csi:
      attacherReplicaCount: 2
      provisionerReplicaCount: 2
      resizerReplicaCount: 2
      snapshotterReplicaCount: 2
      
    # System default settings
    defaultSettings:
      # Replica settings
      defaultReplicaCount: 2
      replicaSoftAntiAffinity: false  # CRITICAL: Must be false for 2-node
      replicaAutoBalance: least-effort  # Balance replicas across nodes
      
      # Taint toleration for storage nodes (required for share-manager pods on tainted nodes)
      taintToleration: "node.longhorn.io/storage=enabled:NoSchedule"
      
      # CRITICAL: Restrict system-managed components (share-manager, instance-manager, etc.) to storage nodes only
      # Without this, share-manager pods can schedule on any node (including cp1 with no storage)
      # Issue: https://github.com/longhorn/longhorn/issues/xxx
      systemManagedComponentsNodeSelector: "node.longhorn.io/storage:enabled"
      
      # NFS client mount options for RWX volumes - use hard mount for reliable writes
      # Use NFSv3 for better compatibility (Unraid uses v3 by default)
      nfsMountOptions: "vers=3,hard,timeo=600,retrans=5"
      
      # Node and disk settings
      createDefaultDiskLabeledNodes: true
      defaultDataPath: /var/lib/longhorn
      defaultDataLocality: best-effort
      
      # Storage settings - adjust for production
      storageOverProvisioningPercentage: 100
      storageMinimalAvailablePercentage: 10
      
      # Node failure handling
      nodeDownPodDeletionPolicy: delete-both-statefulset-and-deployment-pod
      nodeDrainPolicy: block-if-contains-last-replica
      autoDeletePodWhenVolumeDetachedUnexpectedly: true
      
      # Scheduling settings
      disableSchedulingOnCordonedNode: true
      replicaZoneSoftAntiAffinity: false  # We only have one zone per node
      
      # Performance settings
      guaranteedInstanceManagerCPU: 12  # 12% CPU reserved per instance manager
      concurrentReplicaRebuildPerNodeLimit: 2
      
      # System settings
      upgradeChecker: true
      v1DataEngine: true
      v2DataEngine: false  # Stick with stable V1
      
      # Backup settings
      backupTarget: "nfs://192.168.1.29/mnt/cache/longhorn_backup"
      backupTargetCredentialSecret: ""
      
    # Longhorn Manager settings
    longhornManager:
      priorityClass: longhorn-critical
      tolerations:
        - key: node.longhorn.io/storage
          operator: Equal
          value: "enabled"
          effect: NoSchedule
      nodeSelector:
        node.longhorn.io/storage: "enabled"
      
    # Longhorn Driver settings
    longhornDriver:
      priorityClass: longhorn-critical
      tolerations:
        - key: node.longhorn.io/storage
          operator: Equal
          value: "enabled"
          effect: NoSchedule
      nodeSelector: {}  # CSI driver needs to run on all nodes
      
    # Longhorn UI settings
    longhornUI:
      replicas: 1  # Only need 1 for homelab
      priorityClass: longhorn-critical
      tolerations:
        - key: node.longhorn.io/storage
          operator: Equal
          value: "enabled"
          effect: NoSchedule
      nodeSelector:
        node.longhorn.io/storage: "enabled"
      
    # Image settings (use defaults)
    image:
      pullPolicy: IfNotPresent
      
    # Service settings
    service:
      ui:
        type: ClusterIP
        nodePort: null
      manager:
        type: ClusterIP
        nodePort: null
