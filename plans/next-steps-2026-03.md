# Next Steps — Spring 2026 Roadmap

**Date**: March 1, 2026  
**Status**: Planning phase  
**Owner**: Chris

---

## Overview

This plan prioritizes quick wins (backups, dependency automation), observability foundations, then deploys Live TV services with secret management. This order reflects the fact that UI-configured services (Plex, Sonarr, etc.) don't need SOPS, while pluto-for-channels and other Live TV services do.

### Key Insight
Only the **Live TV stack** (pluto-for-channels, EPlusTV, etc.) requires encrypted secrets in git. C# automation apps are optional. This lets you deploy observability and optional tooling before implementing SOPS, then add secrets management just before bringing Live TV online.

---

## Phase 1: Foundation (Weeks 1–2)

### 1.1 Backups — Extend Longhorn Recurring Jobs

**Status**: ✅ COMPLETED (2026-03-01)

**Scope**: Protect all Longhorn volumes (9 total) with nightly automated backups.

**What was delivered** (optimized approach):
- Single consolidated `RecurringJob` named `backup-all-volumes` (not per-app jobs)
- Backs all 9 cluster volumes labeled `recurring-job-group.longhorn.io/default=enabled` in one nightly run
- Schedule: 3:00 AM UTC daily
- Retention: 7 daily backups, with automatic cleanup of oldest
- Destination: `nfs://192.168.1.29:/mnt/cache/longhorn_backup` (incremental snapshots)
- Snapshot naming: `backup-a-{uuid}` (from job name prefix)
- File: `clusters/homelab/infrastructure/longhorn/recurring-backup-all.yaml`

**Why consolidated instead of per-app**: All volumes share the same label group (`default`). Separate per-app jobs would redundantly back all 9 volumes 5 times per night (wasteful). Single job is simpler, faster, and appropriate for homelab.

**Issue & fix during implementation**:
- Initial attempt with 4 app-specific jobs + `groups: []` → found 0 volumes (groups selector was empty)
- Root cause: RecurringJob must specify matching volume group label name in `spec.groups` field
- Solution: Changed to `groups: ["default"]` and consolidated to single job

**Verification** (test run completed):
- Deployed and tested with manual job trigger
- All 9 volumes backed up successfully (~260 MB composite backup)
- First scheduled run: 2026-03-02 at 3:00 AM UTC
- Longhorn UI shows completed backups with `backup-a-` prefix

**Effort**: ~30 min initial + 20 min troubleshooting + 10 min optimization = 60 min total

---

### 1.2 Renovate — Automated Dependency Updates

**Status**: ✅ COMPLETED (2026-03-01)

**Scope**: Keep Flux HelmReleases and container images pinned and up-to-date.

**Why Renovate over Dependabot**: Renovate understands Flux `HelmRelease` chart versions and container tags in k8s manifests. Dependabot does not.

**What was delivered**:
- `renovate.json` config at repo root with Helm + Docker + Kubernetes + Flux managers enabled
- Registry alias: `lscr.io` → `ghcr.io` for linuxserver image compatibility
- Custom package rules for versioning:
  - linuxserver images: strict `X.Y.Z` semver only (ignore full build tags like `1.43.0.10492-121068a07-ls294`)
  - pulsarr: semver only, block beta/node suffixes
  - profilarr: v-prefixed semver
- Flux updates: `dependencyDashboardApproval: true` (shows in dashboard, never auto-creates PR since `gotk-components.yaml` is machine-generated)
- All container images pinned to verified, stable versions
- Mend-hosted Renovate app integrated (no GitHub Actions workflow needed)

**Key learnings** (from config error investigation):
1. **Config preset deprecation**: `config:base` deprecated since Renovate v37 → replaced with `config:recommended`
2. **Invalid top-level keys**: `helm` and `docker` are language category names, not valid manager config objects. Renovate schema rejects them. Use actual manager names: `helmv3`, `dockerfile`, `docker-compose`, `kubernetes`, `flux`, etc.
3. **postUpdateOptions errors**: `yarnDedupeModules` doesn't exist (valid: `yarnDedupeFewer`, `yarnDedupeHighest`). `gomodTidy`, `npmDedupe` irrelevant for pure IaC repos.
4. **Flux gotk-components.yaml special case**: This file is generated by `flux install --export`. Renovate can detect image tag updates but **should not auto-create PRs** for it. Use `dependencyDashboardApproval: true` to require manual checkbox approval before PR creation. Proper upgrade path: `flux install --version=vX.Y.Z --export > clusters/homelab/flux-system/gotk-components.yaml`.
5. **Mend-hosted app redundancy**: GitHub Actions workflow is unnecessary when Mend app is active. Removed to avoid duplicate runs and spurious "Package lookup failures" warnings.

**Verification**:
- Renovate Dependency Dashboard created automatically (Issue #11)
- First detected update: Flux v2.7.5 → v2.8.1 (awaiting manual approval)
- All container images tracked per configured rules
- No validation errors in Mend portal

**Effort**: ~1.5 hours (setup + research + config error debugging).

---

---

## Phase 2: Observability Foundation (Weeks 2–3)

### 2.1 Centralized Logging — VictoriaLogs + Ingress

**Status**: Planning phase (detailed plan: [plans/victoria-logs-deployment.md](plans/victoria-logs-deployment.md))

**Scope**: Centralized log aggregation for media apps (sonarr, plex, decypharr, radarr, etc.) + infrastructure components.

**Why VictoriaLogs over Loki/Prometheus**:
- **Simpler setup**: Single binary StatefulSet vs Loki's DaemonSet + Promtail sidecar complexity
- **Lower resource overhead**: ~500 MB–1 GB RAM vs Loki's 1–2 GB
- **Better high-cardinality handling**: Efficient log compression for pod-specific logs
- **Smaller disk footprint**: ~15 GB/month vs Loki's ~20 GB/month
- **Intuitive queries**: Regex/filter-based vs LogQL's domain-specific language
- **No Grafana dependency**: Built-in VMUI web interface (`http://logs.homelab/vmui`) sufficient for debugging
- **No metrics overhead**: Skip Prometheus entirely; only logs are useful for your homelab

**What gets deployed**:
1. **VictoriaLogs StatefulSet** (1 replica)
   - Container: `victorialogs/victoria-logs`
   - Storage: 100 Gi PVC on `longhorn-simple` (RWO)
   - Retention: 7 days (auto-cleanup of older logs)
   - Memory: 512Mi requests / 1Gi limits
   - Log scrape: Kubelet `/var/log/pods/**` via filesd

2. **Ingress route** — `http://logs.homelab` → VMUI at `:9428/vmui`

3. **Kustomization** — `clusters/homelab/infrastructure/victoria-logs/`

**Implementation steps**:
1. Create HelmRelease + HelmRepository (Victoria Metrics helm charts)
2. Create Ingress (Traefik, route to `:9428`)
3. Create kustomization.yaml
4. Deploy via Flux: `flux reconcile kustomization infrastructure --with-source`
5. Verify logs flowing: Query `{namespace="media"}` in VMUI

**Query examples** (simple, no LogQL needed):
```
{namespace="media"}                    # All media app logs
{pod="sonarr*"}                        # Sonarr pod logs only
{pod=~"(sonarr|radarr).*"}             # Sonarr + Radarr combined
{pod=~".*"} | "error"                  # Text search: all errors across cluster
{pod="plex*"} | "transcode"            # Plex transcoding logs
```

**Storage & retention**:
- PVC size: 100 Gi (6-month buffer at ~15 GB/month)
- Auto-labeled for nightly Longhorn backups (`recurring-job-group.longhorn.io/default=enabled`)
- Node affinity: Preferred on w1 (primary storage node)

**Verification criteria**:
- [ ] VictoriaLogs StatefulSet running (1 replica, Ready)
- [ ] PVC bound to 100 Gi from longhorn-simple
- [ ] Ingress active: `http://logs.homelab/vmui` loads
- [ ] VMUI shows available labels (pod, namespace, container)
- [ ] Can query `{namespace="media"}` and see recent logs from sonarr, radarr, plex, decypharr-*, etc.
- [ ] Logs are < 5 min old for active pods

**Effort**: ~30 min (manifests + deploy + test).

**Blocking dependency for**: None (optional, but strongly recommended before Phase 4/5).

**Future extension**: Add Grafana later if dashboards become needed (independent deployment, not required for basic logging).

---

## Phase 3: C# App Platform + First App (Optional, Weeks 3–5)

### 3.1 Repository: Create `sonarr-utils` repo

**Scope**: First C# app — root folder / tag manager for Sonarr+Radarr.

**Setup**:
1. **New GitHub repo**: `cmpetersen5551/sonarr-utils` (private or public)
2. **Technology stack**:
   - `.NET 9` (or latest LTS)
   - `WorkerService` / `IHostedService` for graceful shutdown
   - `Serilog` for structured logging to stdout
   - `HttpClient` factory pattern for Sonarr/Radarr API calls
   - `IOptions<SonarrSettings>` for config injection

3. **Project structure**:
   ```
   sonarr-utils/
   ├── .github/workflows/
   │   ├── build-push-ghcr.yml         # Build + publish to GHCR on release
   │   └── renovate.json               # Auto-update deps
   ├── src/
   │   ├── SonarrUtils/
   │   │   ├── Program.cs              # DI setup, IHostedService
   │   │   ├── Services/
   │   │   │   ├── SonarrClient.cs     # API wrapper
   │   │   │   ├── RadarrClient.cs     # API wrapper
   │   │   │   └── RootFolderManager.cs # Business logic
   │   │   ├── Models/
   │   │   │   ├── SonarrSettings.cs
   │   │   │   └── RootFolder.cs
   │   │   └── Dockerfile
   │   └── SonarrUtils.Tests/
   ├── .dockerignore
   ├── Dockerfile
   ├── sonarr-utils.sln
   └── README.md
   ```

4. **Dockerfile** (multi-stage):
   ```dockerfile
   FROM mcr.microsoft.com/dotnet/sdk:9.0 AS builder
   WORKDIR /build
   COPY . .
   RUN dotnet publish -c Release -o /app

   FROM mcr.microsoft.com/dotnet/runtime:9.0
   WORKDIR /app
   COPY --from=builder /app .
   ENTRYPOINT ["./SonarrUtils"]
   ```

5. **GitHub Actions workflow** — on release tag:
   - Build multi-arch images (linux/amd64, linux/arm64)
   - Push to GHCR: `ghcr.io/cmpetersen5551/sonarr-utils:v1.0.0`
   - Tag as `:latest`

### 3.2 Business Logic: Root Folder + Tag Manager

**Purpose**: Utility to bulk-update Sonarr/Radarr root folders and tags.

**Example commands**:
```bash
# List all root folders in Sonarr
sonarr-utils list-root-folders --sonarr-url http://sonarr:8989 --api-key <key>

# Tag all series in a root folder
sonarr-utils tag-series \
  --sonarr-url http://sonarr:8989 \
  --api-key <key> \
  --root-folder /mnt/tv \
  --tag "4k-only"

# Move series from one root folder to another
sonarr-utils migrate-root-folder \
  --sonarr-url http://sonarr:8989 \
  --api-key <key> \
  --from /mnt/tv \
  --to /mnt/tv-4k \
  --tag "4k-only"
```

**Implementation approach**:
- Bare minimum: list, tag (no actual move, to avoid breaking things)
- Advanced: dry-run mode → actual execution
- Logging: every operation logged with structured Serilog output

### 3.3 Deploy to K8s

**K8s manifests** (in GitOps repo):
1. `clusters/homelab/apps/media/sonarr-utils/`
   - `namespace.yaml` (or reuse `media` namespace)
   - `kustomization.yaml`
   - `cronjob.yaml` — example CronJob (e.g., daily tag sync)
   - `secret.yaml` (SOPS-encrypted)
     - `SONARR_API_KEY`
     - `SONARR_URL`
     - `RADARR_API_KEY`
     - `RADARR_URL`

2. **CronJob example**:
   ```yaml
   apiVersion: batch/v1
   kind: CronJob
   metadata:
     name: sonarr-utils-sync-tags
   spec:
     schedule: "0 2 * * *"  # Daily 2 AM
     jobTemplate:
       spec:
         template:
           spec:
             containers:
             - name: sonarr-utils
               image: ghcr.io/cmpetersen5551/sonarr-utils:latest
               args: ["tag-sync", "--dry-run"]
               env:
               - name: SONARR_API_KEY
                 valueFrom:
                   secretKeyRef:
                     name: sonarr-utils-secrets
                     key: api-key
               - name: SONARR_URL
                 value: http://sonarr:8989
             restartPolicy: OnFailure
   ```

**Logs**:
- Captured by Loki (Promtail scrapes pod logs)
- Visible in Grafana under `SonarrUtils` app
- Query: `{pod="sonarr-utils-sync-tags-*"}` in Loki

**Verification**:
- Deploy CronJob → wait for next scheduled run (or manually trigger `Job`)
- Check `kubectl logs pod/sonarr-utils-sync-tags-xxxxx`
- Verify logs appear in Loki/Grafana

**Effort**: ~8–12 hours (C# development + testing + integration).

---

## Phase 4: Secret Management with SOPS (Week 5)

### 4.1 SOPS + age Secret Management

**Scope**: Enable encrypted secrets in git, Flux-native decryption at deploy time.

**Why now**: Live TV services (pluto-for-channels, EPlusTV, etc.) require storing credentials securely in git. Implement SOPS immediately before deploying them.

**Implementation**:
1. **Generate age keypair** locally:
   ```bash
   age-keygen -o age.key
   ```

2. **Create K8s Secret from private key** (bootstrap):
   ```bash
   kubectl create secret generic sops-age \
     --from-file=age.agekey=age.key \
     -n flux-system
   ```

3. **Create `.sops.yaml` at repo root**:
   ```yaml
   creation_rules:
     - path_regex: clusters/.*\.ya?ml$
       encrypted_regex: ^(data|stringData)$
       key_groups:
         - age:
             - <PUBLIC_KEY_OUTPUT_FROM_age-keygen>
   ```

4. **Create Flux `SecretProviderClass`** (or direct `SecretProviderClass/Secret` resources):
   - Reference `sops-age` secret in `flux-system` namespace
   - Flux automatically decrypts SOPS-sealed secrets on reconciliation

5. **Encrypt first secret** (example: Pluto credentials):
   ```bash
   sops --encrypt clusters/homelab/apps/media/pluto-for-channels-secret.yaml > clusters/homelab/apps/media/pluto-for-channels-secret.enc.yaml
   ```

6. **Add to `.gitignore`**:
   ```
   age.key
   *.dec.yaml
   ```

**Reference files to create**:
- `.sops.yaml` (repo root)
- Example encrypted secret in `clusters/homelab/apps/media/`
- Flux `Kustomization` to auto-decrypt

**Documentation**:
- How to add a new secret (edit in plaintext, encrypt, commit)
- How to decrypt locally for review (one-time before commit)
- Age key recovery procedures

**Verification**:
- Encrypt a test value → commit → Flux reconciles → secret appears in cluster
- Verify plaintext secret is NOT in git history
- Test pod can read the secret environment variable

**Effort**: ~1–2 hours. Done once, unlocks Live TV deployment.

---

### 5.1 Channels DVR

**Container**: `fancybits/channels-dvr`  
**Port**: 8089  
**Storage**:
- Config: Longhorn RWO, 20Gi, `config-channels-dvr`
- Recordings: existing `pvc-media-nfs` (Unraid, RWX)

**Manifests**:
- `clusters/homelab/apps/media/channels-dvr/`
  - `namespace.yaml` (reuse `media`)
  - `kustomization.yaml`
  - `pvc-config.yaml` (Longhorn RWO, 20Gi)
  - `statefulset.yaml`
  - `service.yaml` (ClusterIP)
  - `service-headless.yaml`
  - `ingress.yaml` (Traefik, `channels.homelab`)

**StatefulSet key details**:
- Affinity: `preferredDuringScheduling` + `required` for w1/w2 (follow `sonarr` pattern from `clusters/homelab/apps/media/sonarr/statefulset.yaml`)
- Volume mounts:
  - `/opt/channels` → `config-channels-dvr` PVC
  - `/mnt/recordings` → `pvc-media-nfs` (mount path for DVR recordings)
- Resource requests: 2 CPU, 4Gi RAM (conservative; adjust after first run)

### 5.2 pluto-for-channels

**Container**: `jonmaddox/pluto-for-channels`  
**Port**: 8080  
**Storage**: Stateless (no PVC needed)
**Secrets**: `PLUTO_USERNAME`, `PLUTO_PASSWORD` (SOPS-encrypted secret)

**Manifests**:
- `clusters/homelab/apps/media/pluto-for-channels/`
  - `kustomization.yaml`
  - `deployment.yaml` (lightweight, no affinity needed)
  - `service.yaml` (ClusterIP)
  - `ingress.yaml` (Traefik, `pluto.homelab`)
  - `secret.yaml` (SOPS-encrypted)

**Environment variables**:
- `PLUTO_USERNAME` → from secret
- `PLUTO_PASSWORD` → from secret
- `START=10000` (channel numbering)

### 5.3 EPlusTV

**Container**: `tonywagner/eplustv`  
**Port**: 8000  
**Storage**: Longhorn RWO, 5Gi, `config-eplustv`
**Secrets**: Sports provider credentials (ESPN+, FloSports, etc. — user defines)

**Manifests**:
- `clusters/homelab/apps/media/eplustv/`
  - `kustomization.yaml`
  - `pvc-config.yaml` (Longhorn RWO, 5Gi)
  - `deployment.yaml`
  - `service.yaml`
  - `ingress.yaml` (Traefik, `eplustv.homelab`)
  - `secret.yaml` (SOPS-encrypted, sports provider env vars)

### 5.4 Dispatcharr

**Container**: `ghcr.io/dispatcharr/dispatcharr` (all-in-one mode)  
**Port**: 9191  
**Storage**: Longhorn RWO, 10Gi, `data-dispatcharr`

**Manifests**:
- `clusters/homelab/apps/media/dispatcharr/`
  - `kustomization.yaml`
  - `pvc-data.yaml` (Longhorn RWO, 10Gi)
  - `deployment.yaml`
  - `service.yaml`
  - `ingress.yaml` (Traefik, `dispatcharr.homelab`)

### 5.5 Teamarr

**Container**: `ghcr.io/pharaoh-labs/teamarr`  
**Port**: 9195  
**Storage**: Longhorn RWO, 5Gi, `data-teamarr`

**Manifests**:
- `clusters/homelab/apps/media/teamarr/`
  - `kustomization.yaml`
  - `pvc-data.yaml` (Longhorn RWO, 5Gi)
  - `deployment.yaml`
  - `service.yaml`
  - `ingress.yaml` (Traefik, `teamarr.homelab`)

### 5.6 Patch: Update `media/kustomization.yaml`

Add all five new services to the base kustomize file:
```yaml
resources:
  - channels-dvr/kustomization.yaml
  - pluto-for-channels/kustomization.yaml
  - eplustv/kustomization.yaml
  - dispatcharr/kustomization.yaml
  - teamarr/kustomization.yaml
```

### 5.7 Integration: Channels DVR + Custom Channels

**Manual setup in Channels app**:
1. Open Channels DVR admin UI (`http://channels.homelab:8089`)
2. Add custom channel sources:
   - pluto-for-channels: `http://pluto-for-channels:8080/epg.xml`
   - EPlusTV: `http://eplustv:8000/xmltv.xml`
   - Dispatcharr: M3U/EPG from Dispatcharr admin UI
   - Teamarr: Sports EPG

**Backup**: Channels DVR config includes custom channel mappings; backed up nightly via Longhorn.

**Verification**:
- All 5 containers running in `media` namespace
- All services accessible via ingress routes
- Channels DVR sees custom channels from pluto-for-channels
- Recordings land on Unraid NFS (verify via `ls /mnt/recordings`)
- Loki captures all service logs

**Effort**: ~4–6 hours (manifests + testing).

---

## Delivery Order & Dependencies

```
Phase 1: Backups + Renovate ✅
    ↓ (independent)
Phase 2: Monitoring (can overlap with Phase 1)
    ↓
Phase 3: C# Apps [OPTIONAL — skip if not needed yet]
    ↓ (independent after Phase 2)
Phase 4: SOPS (right before Live TV)
    ↓
Phase 5: Live TV stack (depends on SOPS from Phase 4)
```

**Parallelizable**:
- Phase 1 + Phase 2 can overlap
- Phase 3 is optional and can be skipped entirely
- Phase 4 (SOPS) immediately precedes Phase 5 (Live TV)

---

## Timeline Estimate

| Phase | Effort | Slack | Total |
|-------|--------|-------|-------|
| 1: Backups | 0.5 h | 0.5 h | **1 h** |
| 1: Renovate | 0.75 h | 0.25 h | **1 h** |
| 2: Monitoring | 2.5 h | 1 h | **3.5 h** |
| 3: C# apps (optional) | 10 h | 2–4 h | **12–14 h** |
| 4: SOPS + age | 1.5 h | 0.5 h | **2 h** |
| 5: Live TV (5 services) | 4 h | 1–2 h | **5–7 h** |
| **Total (skipping Phase 3)** | ~9 h | ~3 h | **~12 h (1–2 weeks)** |
| **Total (including Phase 3)** | ~19 h | ~5 h | **~24 h (3–4 weeks part-time)** |

---

## Summary

1. **Backups + Renovate** (Week 1): Fast wins, no dependencies ✅
2. **Monitoring** (Week 2): Observability for everything that follows
3. **C# platform** (Weeks 3–5): Optional — invest in tooling if needed, reusable pattern
4. **SOPS** (Week 5): Just-in-time secret management for Live TV services
5. **Live TV** (Weeks 5–7): Deploy 5 services with SOPS-encrypted secrets, integrate with Channels DVR

**Recommended path**: Skip Phase 3 for now, focus on Phases 1, 2, 4, 5. This gets you observability + Live TV in ~2 weeks instead of 4.

