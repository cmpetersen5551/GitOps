# Options Status Summary ‚Äî What's Been Tried, What Remains

**Date**: 2026-02-28  
**Purpose**: Quick reference showing which alternatives have been explored and why they were eliminated.

---

## Timeline of Approaches (Feb 2026)

```
EA96938  ‚ùå kubelet-level NFS volume mounts
  ‚îî‚îÄ Issue: kubelet uses host DNS, doesn't resolve ClusterIP
  
275e46f  ‚ùå Raw ClusterIP for kubelet NFS
  ‚îî‚îÄ Issue: kube-proxy IPs don't work in host network namespace

7FF8B89  ‚ùå Fix NFS export path (rclone serve nfs /mnt/dfs exports as /)
  ‚îî‚îÄ Made some progress but still fundamentally broken

8D10195  ‚ö†Ô∏è In-pod dfs-mounter sidecar with emptyDir mount propagation
  ‚îî‚îÄ Architectural idea good, but ran into peer group isolation

82695f8  ‚ö†Ô∏è Attempt privileged capability for Bidirectional propagation
  ‚îî‚îÄ Didn't solve the deeper peer group issue

C24819C  ‚ùå memory-backed emptyDir (per Kubernetes docs)
  ‚îî‚îÄ FAILED in practice: kubelet creates separate peer groups per container anyway
  ‚îî‚îÄ Confirmed by inspecting /proc/PID/mountinfo: peer group 549 vs 303 disconnected

6FC2751  ‚úÖ Colocate rclone in SAME container as Decypharr
  ‚îî‚îÄ SUCCESS: Same mount namespace = no propagation needed
  ‚îî‚îÄ rclone serve nfs works here

7E0DBF4+ ‚ö†Ô∏è Extensive NFS mount option iterations (25+ commits)
  ‚îî‚îÄ ISSUE: emptyDir sidecar + NFS deadlock observed (later understood as peer group limitation, not NFS bug)
  ‚îî‚îÄ Also discovered: `mountpoint -q` false-positive on emptyDir creates tmpfs

6762381  ‚ùå Pivot to SFTP + sshfs client
  ‚îî‚îÄ CRITICAL DISCOVERY: sshfs is FUSE-based!
  ‚îî‚îÄ Recreates the original peer group propagation problem
  ‚îî‚îÄ sshfs FUSE mount in sidecar never appeared in main container

767D937  ‚ùå Switch to CIFS in DaemonSet (rclone serve smb doesn't exist in v1.73.1)
  ‚îî‚îÄ rclone v1.73.1 has NO serve smb subcommand
  ‚îî‚îÄ Immediately abandoned (next commit)

EDD0D35  ‚ö†Ô∏è Switch back to NFS but use DaemonSet architecture instead of sidecars
  ‚îî‚îÄ DaemonSet runs on each host, not per container
  ‚îî‚îÄ mount -t nfs (kernel mount, not FUSE!) ‚Üê This is the key insight
  ‚îî‚îÄ Propagates through hostPath correctly
  ‚îî‚îÄ WORKS! But...

(Next 25 commits) ‚ö†Ô∏è NFS DaemonSet works until Feb 25, then issues surface
  ‚îî‚îÄ mount detection bugs (mountpoint -q false-positive on hostPath)
  ‚îî‚îÄ NFS option tuning attempts

2026-02-26  ‚úÖ PIVOT to SMB/CIFS DaemonSet
  ‚îî‚îÄ CIFS kernel client has automatic server reconnect (NFS doesn't)
  ‚îî‚îÄ NFS soft mounts become permanently stale when server restarts
  ‚îî‚îÄ CIFS solves the stale mount problem
  ‚îî‚îÄ Commits: `767d937` ‚Üí `edd0d35` ‚Üí actual CIFS deployment

2026-02-28  üîß Add LD_PRELOAD nlink shim to Samba
  ‚îî‚îÄ Samba treating st_nlink=0 as deleted inode
  ‚îî‚îÄ Fix: patch stat() syscalls to set st_nlink=2 (dirs) or 1 (files)
  ‚îî‚îÄ Commits: `64f462b`, extended to files in later commit
  ‚îî‚îÄ WORKING SOLUTION ‚úÖ
```

---

## All Alternatives Evaluated

### By Status

#### ‚ùå Ruled Out Completely (Won't Revisit)

| Option | Attempted | Reason | Impact |
|--------|-----------|--------|--------|
| **Kubelet-level NFS** | ‚úÖ Yes (Feb) | DNS + IP namespace issues (unfixable in k8s) | Couldn't mount at all |
| **Direct hostPath FUSE propagation** | ‚úÖ Yes (early Feb) | FUSE mounts don't propagate through container peer groups (fundamental k8s limitation) | No data access |
| **EmptyDir sidecar + rclone NFS** | ‚úÖ Yes (Feb) | Peer group isolation (kubelet creates separate peer groups per container) | Empty mount on sidecar |
| **Memory-backed emptyDir** | ‚úÖ Yes (Feb) | Same peer group issue despite Kubernetes docs suggesting it should work | Confirmed with /proc/mountinfo |
| **SFTP + sshfs** | ‚úÖ Yes (Feb) | sshfs is FUSE ‚Üí recreates peer group problem | Was trying to escape FUSE but reintroduced it |
| **SeaweedFS CSI + decypharr** | ‚úÖ Researched (archived) | CSI drivers provision volumes but don't solve FUSE sharing between pods | Doesn't apply to our problem |
| **Zurg + Unraid** | ‚ùå Not tried (user constraint) | User committed to Decypharr; also Unraid SPOF | Ruled out by requirements |
| **STRM files only** | ‚ùå Not viable | Plex doesn't reliably support .strm files; loses byte-access for GPU transcode | Not compatible with Plex |

#### ‚ö†Ô∏è Worked But Abandoned

| Option | Attempted | Result | Why Abandoned |
|--------|-----------|--------|---------------|
| **NFS DaemonSet** | ‚úÖ Yes (Feb 25-26) | ‚úÖ Worked (rclone serve nfs + kernel NFS client) | NFS soft mounts become permanently stale when server (decypharr pod) restarts; requires pod restart to recover; unacceptable for Plex |

#### ‚úÖ Current Solution

| Option | Status | Details |
|--------|--------|---------|
| **CIFS DaemonSet** | ‚úÖ **In Production** | Deployed Feb 26, 2026. CIFS has automatic server reconnect (solves NFS stale mount problem). Requires LD_PRELOAD shim for st_nlink=0 bug. |

#### ‚ùì Not Yet Explored (Still Viable)

| Option | Category | Complexity | Timeline | Notes |
|--------|----------|-----------|----------|-------|
| **Option D: Fix go-fuse upstream** | Root cause fix | Medium | 2-4 weeks | PR to hanwen/go-fuse to report correct st_nlink. Would eliminate LD_PRELOAD shim. Recommended next step. |
| **Option E: Decypharr CSI Driver** | Rearchitecture | Very High | 2-3 weeks prototype | Would be Kubernetes-native, scalable to many apps. Overkill for 2-3 apps. Consider if scaling to 5+. |
| **Option F: WebDAV export** | Alternative protocol | Medium | 1-2 weeks test | Different protocol layer (HTTP vs SMB). Might bypass st_nlink issue. Performance lower than kernel mounts. Unproven with Plex. |
| **Option G: Patch Samba directly** | Workaround | Low | 2-3 hours | Build Samba with custom patch instead of LD_PRELOAD. Not recommended (LD_PRELOAD is simpler). |

---

## Key Technical Insights (Learned the Hard Way)

### 1. FUSE vs. Kernel VFS Mounts
- **FUSE mounts** (`sshfs`, `s3fs`, `goofys`, `go-fuse`) ‚Äî Do NOT propagate through Kubernetes container mount peer groups
- **Kernel VFS mounts** (`mount -t nfs`, `mount -t cifs`, `mount -t ext4`) ‚Äî Propagate correctly through `hostPath: Bidirectional`

**This is why NFS and CIFS DaemonSet approaches work but sidecar approaches don't.**

### 2. Kubernetes Mount Propagation Reality
- Kubernetes docs recommend `emptyDir: {medium: Memory}` (tmpfs) for Bidirectional propagation
- In practice: `kubelet` creates **separate bind-mounts** for each container of the same tmpfs
- Each bind-mount lands in a different mount peer group (ID 549 vs 303 observed)
- **FUSE mounts don't cross peer group boundaries**, so propagation fails
- This is a fundamental k8s design, not a configuration error

### 3. Mount Detection Bug
- `mountpoint -q /mnt/dfs` on a `hostPath` volume is ALWAYS true
- hostPath volumes are kernel mountpoints themselves
- For accurate "is my desired filesystem mounted", use: `grep -q 'specific-fs-identifier /mnt/dfs' /proc/mounts`

### 4. NFS Soft Mount Stale State
- When NFS server disappears (pod restart), kernel marks mount as broken  
- Any I/O returns `ESTALE` immediately
- Retry logic doesn't reconnect; mount is permanently broken
- Only fix: `umount` + `mount` again (pod restart in k8s context)
- SMB/CIFS auto-reconnects; no pod restart needed

### 5. rclone serve smb Doesn't Exist
- `rclone serve` subcommands: dlna, docker, ftp, http, **nfs**, restic, s3, sftp, webdav
- NO `serve smb` ‚Äî this is a community request, not implemented in official rclone
- Initially assumed it existed (commit 767d937), immediately abandoned (commit edd0d35)

### 6. Samba's st_nlink=0 Semantics
- Samba 4.x treats st_nlink=0 as an **unlinked/deleted inode marker**
- Not just a file metadata field; it's inode deletion state
- Any inode (file or directory) with st_nlink=0 returns `NT_STATUS_OBJECT_NAME_NOT_FOUND` at the SMB protocol level
- This is Samba-specific behavior (not part of SMB spec), triggered by FUSE returning st_nlink=0

---

## Lessons for Future Storage/FUSE Work

1. **Always verify if a filesystem is FUSE or kernel-based before choosing it**
   - `sshfs` is FUSE (learned the hard way at attempt 6)
   - `davfs` is FUSE (if you try WebDAV approach)
   - `s3fs`, `goofys`, most userspace filesystems are FUSE

2. **FUSE + Kubernetes + mount propagation = architectural mismatch**
   - Don't try to share FUSE mounts between containers
   - Re-export FUSE as a network protocol (NFS, SMB, WebDAV) instead
   - Use kernel mounts to access the re-export

3. **NFS soft mounts have stale state issues; CIFS auto-reconnects**
   - If you need reliability during server restarts, use CIFS/SMB
   - NFS is simpler infrastructure (no Samba), but operationally more fragile

4. **Test mount detection scripts carefully**
   - `mountpoint -q` is not reliable on hostPath volumes
   - Use `grep /proc/mounts` to detect specific filesystem types
   - Test in actual container environment, not just bash

5. **When making Samba workarounds, document the root cause**
   - The st_nlink=0 issue is go-fuse-specific, not a general Samba limitation
   - If fixed upstream, the workaround becomes unnecessary

---

## What Would Happen If You Revisited Each Option Now (Feb 28)

### Option A: Try Direct hostPath FUSE Again
**Outcome**: Same failure as early February. FUSE propagation through container mount namespaces is fundamentally broken in Kubernetes. No new insights would change this.

**Not recommended**: Don't waste time retrying.

### Option B: Switch Back to NFS (without fixes)
**Outcome**: Same as Feb 25. NFS DaemonSet works until decypharr pod restarts, then Sonarr/Radarr get `ESTALE` errors and need to restart. Plex mid-stream pause + restart = session lost.

**Not recommended**: CIFS solves this problem better.

### Option C: Revisit SeaweedFS CSI
**Outcome**: SeaweedFS CSI is for distribu proofs, not FUSE sharing. You'd still have the problem of how Decypharr shares its FUSE with other pods.

**Not recommended**: Wrong tool for this job.

### Option D: Fix go-fuse (UP STR EAM)
**Outcome**: ‚úÖ Viable. Could eliminate LD_PRELOAD shim. Worth trying.

**Recommended**: Open PR with hanwen/go-fuse maintainers.

### Option E: Build a CSI Driver
**Outcome**: ‚úÖ Viable if you scale. Would be clean, Kubernetes-native. But overkill for 2-3 apps.

**Recommended**: Only if you add 5+ RealDebrid-dependent apps.

### Option F: WebDAV
**Outcome**: ‚ùì Unknown. Different protocol might avoid st_nlink issues, but performance trade-offs. Unproven with Plex symlinks.

**Recommended**: Low priority; CIFS + fix go-fuse is better path forward.

---

## Decision Framework (Updated)

**If you want to keep status quo**: ‚úÖ Current SMB/CIFS is stable. Monitor for go-fuse upstream fix.

**If you want to remove C shim**: ‚Üí **Option D** (Fix go-fuse, 2-4 week timeline)

**If you want long-term cleanliness**: ‚Üí **Option D** (upstream), then **Option E** if you scale beyond 3 apps

**If you want "Kubernetes-native" today**: ‚Üí **Option E** (CSI), but high implementation cost

**If you want to prove concept of alternatives**: ‚Üí Test **Option F** (WebDAV) in staging, but don't roll to production unless it's dramatically better

---

## Recommendation Summary

‚úÖ **Current approach (SMB/CIFS + LD_PRELOAD) is the right choice.**

**Keep it and**:
1. Test HA failover + CIFS auto-reconnect behavior (1-2 hours)
2. Research go-fuse upstream (1-2 hours)
3. Open PR with hanwen/go-fuse if issue not already reported (4 hours)
4. Once merged, remove LD_PRELOAD shim (1-2 hours)

**Don't**:
- ‚ùå Try direct hostPath FUSE again (won't work)
- ‚ùå Switch back to NFS (CIFS is better)
- ‚ùå Rush to CSI driver (overkill for 2-3 apps)
- ‚ùå Build SMB from scratch (rclone has no serve smb; use samba package instead)

---

**Last Updated**: 2026-02-28  
**Status**: All viable paths identified; current approach validated
